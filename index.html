<!DOCTYPE html>
<html>
<head>

  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>UniVaR</title>
  <link rel="icon" type="image/x-icon" href="static/images/logo_caire-small">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link href="//netdna.bootstrapcdn.com/font-awesome/3.2.1/css/font-awesome.css" rel="stylesheet">


  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">UniVaR: LLM Human Value Representation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://samuelcahyawijaya.github.io/" target="_blank">Samuel Cahyawijaya</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://chendelong.world/" target="_blank">Delong Chen</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://yjbang.github.io/" target="_blank"> Yejin Bang</a><sup>*</sup>,</span>
                    <span class="author-block">Leila Khalatbari,</span>
                    <br>
                    <span class="author-block">
                      <a href="https://bryanwilie.github.io/" target="_blank"> Bryan Wilie</a><sup>*</sup>,</span>
                      <span class="author-block">
                        Ziwei Ji</a>,</span>
                        <span class="author-block">
                          Etsuko Ishii</a>,</span>
                          <span class="author-block">
                            <a href="https://pascale.home.ece.ust.hk/" target="_blank">Pascale Fung</a><sup>*</sup></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">HKUST<br>NAACL 2025</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2404.07900" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/HLTCHKUST/UniVaR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="http://eez116.ece.ust.hk:7861" target="_blank"
                  class="external-link button is-normal is-rounded">
                  <!-- <span class="icon"><i class="fa-eye"></i></span> -->
                  <span>Visualizer</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="columns"> 
        <div class="column">

            <img src="static/images/univar-demo-gif.gif" alt="gif" style="width: 50%; height: auto;" class="center">
        </div>
      </div>
    <h2 class="subtitle has-text-centered">
        LLM Human Value visualizer using UniVaR. The red color shows the tested LLM's value position. You can test your model with our visualizer.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce <strong>UniVaR</strong>, a high-dimensional neural representation capable of encapsulating symbolic human value distributions within Large Language Models (LLMs). UniVaR is a continuous, scalable representation learned in a self-supervised manner from value-relevant responses of 8 LLMs and evaluated across 15 open-source and commercial LLMs. By visualizing UniVaR, we shed light on the differences and similarities in value systems within LLMs, offering insights into their underlying value systems and their prioritization across different languages. This ultimately drives forward transparency and accountability in the design and deployment of LLMs. </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Exploring the Human Value Embedding Map in LLMs
          </h2>
          <div class="columns">
            <div class="column">
                <img src="static/images/cultural_map.png" alt="Figure a" style="width: 80%; height: auto;" class="center">
            </div>
          </div>

          <div class="level-set has-text-justified">
            <p>
              UniVaR is trained through a surrogate task, called value embedding learning, to learn a compact representation that contains maximized mutual information with value-relevant aspects of LLMs while discarding other confounding factors as much as possible. With the incorporation of value-eliciting QAs, UniVaR applies multi-view self-supervised learning by maximizing mutual information across views to ensure capturing the shared value-relevant aspects across the two views while excluding other non-shared factors.
            <br>
            </p>
          </div>
          <!-- New content for figures and caption -->
          <!-- <div class="caption">
           <p>Value Map for LLMs</p>
          </div> -->
          <!-- End new content -->

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">UniVaR is not a Sentence Embedding
          </h2>
          <!-- <div class="level-set has-text-justified">
            <p>
              To solve this task we present DSiRe (Dataset Size Recovery), a method that recovers the dataset size directly from the LoRA weights. DSiRe leverages the spectrum of LoRA matrices to predict the number of samples used for fine-tuning. Our approach is simple yet highly effective, achieving high accuracy across a diverse datasets and models.
            <br>
            </p>
          </div> -->
          <!-- New content for figures and caption -->
          <div class="column">
            <img src="static/images/qa_pipeline.png" alt="experience" class="blend-img-background center-image" style="max-width: 90%; height: auto; margin-left: 5px; margin-bottom: 20px;" />
            <p>
            To ensure minimal sharing of linguistics aspect across views, we translate all the value-eliciting QAs to English and perform paraphrasing to avoid language-specific markers and increase the linguistics diversity. UniVaR displays a strong capability surpassing all baselines by ~15% k-NN accuracy and 10-15% linear probing accuracy@10 on the LLM value identification task. While, word and sentence embedding representations perform poorly indicating that there are significant differences between value representations from UniVaR and existing embedding representations.
            </p>
          </div>
          <!-- End new content -->

        </div>
      </div>
    </div>
  </div>
</section>





<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/NAACL__2025___UniVaR_Poster.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{cahyawijaya2024highdimension,
        title={High-Dimension Human Value Representation in Large Language Models}, 
        author={Samuel Cahyawijaya and Delong Chen and Yejin Bang and Leila Khalatbari and Bryan Wilie and Ziwei Ji and Etsuko Ishii and Pascale Fung},
        year={2024},
        eprint={2404.07900},
        archivePrefix={arXiv},
        primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
